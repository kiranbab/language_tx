# Transformer Components in PyTorch
This README outlines the implementation of various components used in the Transformer architecture, as described in the paper "Attention is All You Need" by Vaswani et al. The implementation is done using PyTorch, a popular deep learning library. The components covered include Input Embedding, Positional Embedding, Layer Normalization, FeedForward Block, Multi-Head Attention Block, and Residual Connection. These components are foundational for building Transformer models for tasks like language translation, text summarization, and more.

## Dependencies
1.PyTorch
2.Math
Ensure you have PyTorch installed in your environment to use these components. You can install PyTorch by following the instructions on the official website.